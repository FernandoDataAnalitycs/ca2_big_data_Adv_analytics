{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea0eea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.64.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2225c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3db2c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5dc9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python3-dotenv # command did not work because of the message \"error: externally-managed-environment\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0483cbf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv('.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0acc1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# mysql variables\n",
    "USER_MYSQL = os.getenv('USER_MYSQL')\n",
    "PASSWORD_MYSQL = os.getenv('PASSWORD_MYSQL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c4abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Connect to MySQL database using SQLAlchemy\n",
    "link = f\"mysql+mysqlconnector://{USER_MYSQL}:{PASSWORD_MYSQL}@localhost/ca2_tweets\"\n",
    "engine = create_engine(link)\n",
    "\n",
    "# Specify the chunk size\n",
    "chunksize = 10000\n",
    "\n",
    "# Initialize the start index\n",
    "start_index = 0\n",
    "\n",
    "# loop to upload the csv into mysql: \n",
    "#for chunk in pd.read_csv(\"ProjectTweets.csv\", chunksize=chunksize):\n",
    "#    # Insert chunk into MySQL database\n",
    "#    chunk.to_sql(name=\"ProjectTweets\", con=engine, if_exists=\"append\", index=False)\n",
    "#    \n",
    "#    # Increment start index\n",
    "#    start_index += len(chunk)\n",
    "#    print(f\"Uploaded {start_index} rows\")\n",
    "\n",
    "# Dispose of the engine\n",
    "engine.dispose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad3a0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in ca2_tweets database:\n",
      "ProjectTweets\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=USER_MYSQL,\n",
    "    password=PASSWORD_MYSQL,\n",
    "    database=\"ca2_tweets\"\n",
    ")\n",
    "\n",
    "# Create a cursor object to execute SQL queries\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute SQL query to show tables\n",
    "cursor.execute(\"SHOW TABLES\")\n",
    "\n",
    "# Fetch all rows from the result set\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Display the tables\n",
    "print(\"Tables in ca2_tweets database:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "# Close cursor and MySQL connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4fcdd39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 15:07:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 1:>                                                          (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- ids: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:=========>                                                 (1 + 5) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ca2\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "df = spark.read.csv(\"hdfs://localhost:9000/ca2_bd_ad/ProjectTweets.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Print DataFrame schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c96c5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+--------+---------------+--------------------+\n",
      "|index|       ids|                date|    flag|           user|                text|\n",
      "+-----+----------+--------------------+--------+---------------+--------------------+\n",
      "|    0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|    2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|    3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|    4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+-----+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d746d2",
   "metadata": {},
   "source": [
    "## EDA :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d82845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:=======================================>                   (4 + 2) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in column  index  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  index\n",
      "Null values in column  ids  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  ids\n",
      "Null values in column  date  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  date\n",
      "Null values in column  flag  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  flag\n",
      "Null values in column  user  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  user\n",
      "Null values in column  text  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  text\n"
     ]
    }
   ],
   "source": [
    "# check missing values:\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "for name in df.columns:\n",
    "    # Check if there are null values in a specific column\n",
    "    null_values_count = df.filter(col(name).isNull()).count()\n",
    "\n",
    "    # Check if there are non-null values in a specific column\n",
    "    non_null_values_count = df.filter(col(name).isNotNull()).count()\n",
    "\n",
    "    # Print the counts\n",
    "    print(\"Null values in column \",name,\" :\", null_values_count)\n",
    "    print(\"Non-null values in column \",\" :\", non_null_values_count)\n",
    "    print(\"column name is : \",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95517573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    flag|\n",
      "+--------+\n",
      "|NO_QUERY|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_flag_column = df.select(\"flag\").distinct()\n",
    "check_flag_column.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e2e3ae",
   "metadata": {},
   "source": [
    "#### column \"flag\" has just 1 value, so it is not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b82d736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 42:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|       ids|count|\n",
      "+----------+-----+\n",
      "|1468544973|    2|\n",
      "|1690908358|    2|\n",
      "|1834777946|    2|\n",
      "|1882160717|    2|\n",
      "|1965601765|    2|\n",
      "|1982434182|    2|\n",
      "|1685304801|    2|\n",
      "|1686371908|    2|\n",
      "|1957194329|    2|\n",
      "|1969964899|    2|\n",
      "|1974268607|    2|\n",
      "|1556266702|    2|\n",
      "|1752414405|    2|\n",
      "|1824843992|    2|\n",
      "|1881996107|    2|\n",
      "|1983726537|    2|\n",
      "|1973518810|    2|\n",
      "|1983706615|    2|\n",
      "|1556925184|    2|\n",
      "|1977761720|    2|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 42:=========>                                                (1 + 5) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def check_repeated_values(df, column_name):\n",
    "    return df.groupBy(column_name).count().filter(col(\"count\") >= 2)\n",
    "\n",
    "repeated_values_df = check_repeated_values(df, \"ids\")\n",
    "\n",
    "# Show the DataFrame containing rows with repeated values\n",
    "repeated_values_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a07989ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with repeated values: 1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the number of rows with repeated values\n",
    "num_repeated_rows = repeated_values_df.count()\n",
    "print(\"Number of rows with repeated values:\", num_repeated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c49029c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after removing duplicates:\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates based on the \"id\" column\n",
    "df_no_duplicates = df.dropDuplicates([\"ids\"])\n",
    "\n",
    "# Print DataFrame without duplicates\n",
    "print(\"\\nDataFrame after removing duplicates:\")\n",
    "#df_no_duplicates.show().count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "975e316a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1598315"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_duplicates.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aeb3c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 57:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|date                        |\n",
      "+----------------------------+\n",
      "|Mon Apr 06 22:20:37 PDT 2009|\n",
      "|Mon Apr 06 22:20:40 PDT 2009|\n",
      "|Mon Apr 06 22:21:30 PDT 2009|\n",
      "|Mon Apr 06 22:21:39 PDT 2009|\n",
      "|Mon Apr 06 22:22:05 PDT 2009|\n",
      "|Mon Apr 06 22:22:06 PDT 2009|\n",
      "|Mon Apr 06 22:22:26 PDT 2009|\n",
      "|Mon Apr 06 22:22:47 PDT 2009|\n",
      "|Mon Apr 06 22:22:48 PDT 2009|\n",
      "|Mon Apr 06 22:22:49 PDT 2009|\n",
      "+----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The first 10 values, including the whole text, of the \"date\" column\n",
    "df_no_duplicates.select(\"date\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a0755bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=========>                                                (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, unix_timestamp\n",
    "\n",
    "# Set Spark configuration for timeParserPolicy\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Convert the string-formatted \"date\" column to a timestamp\n",
    "df_with_timestamp = df_no_duplicates.withColumn(\"timestamp\", unix_timestamp(\"date\", \"EEE MMM dd HH:mm:ss zzz yyyy\").cast(\"timestamp\"))\n",
    "\n",
    "# Convert the timestamp to date format\n",
    "df_with_date = df_with_timestamp.withColumn(\"date\", to_date(\"timestamp\"))\n",
    "\n",
    "# Show the first 10 values of the \"date\" column after conversion\n",
    "df_with_date.select(\"date\").show(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdf145a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- ids: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confirm the conversion to date type in column: \n",
    "df_with_date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bedc7e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 63:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+--------+--------------+--------------------+-------------------+\n",
      "|index|       ids|      date|    flag|          user|                text|          timestamp|\n",
      "+-----+----------+----------+--------+--------------+--------------------+-------------------+\n",
      "|   20|1467813985|2009-04-07|NO_QUERY|        quanvu|@alydesigns i was...|2009-04-07 05:20:37|\n",
      "|   22|1467814119|2009-04-07|NO_QUERY|     cooliodoc|@angry_barista I ...|2009-04-07 05:20:40|\n",
      "|   37|1467817374|2009-04-07|NO_QUERY|       ajaxpro|@MissXu sorry! be...|2009-04-07 05:21:30|\n",
      "|   39|1467818007|2009-04-07|NO_QUERY|Anthony_Nguyen|Bed. Class 8-12. ...|2009-04-07 05:21:39|\n",
      "|   45|1467819650|2009-04-07|NO_QUERY|     antzpantz|@Viennah Yay! I'm...|2009-04-07 05:22:05|\n",
      "+-----+----------+----------+--------+--------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_with_date.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef75efc",
   "metadata": {},
   "source": [
    "## column \"text\" : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b3d825b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 66:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+--------+--------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|index |ids       |date      |flag    |user          |text                                                                                                                                      |timestamp          |\n",
      "+------+----------+----------+--------+--------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|20    |1467813985|2009-04-07|NO_QUERY|quanvu        |@alydesigns i was out most of the day so didn't get much done                                                                             |2009-04-07 05:20:37|\n",
      "|22    |1467814119|2009-04-07|NO_QUERY|cooliodoc     |@angry_barista I baked you a cake but I ated it                                                                                           |2009-04-07 05:20:40|\n",
      "|37    |1467817374|2009-04-07|NO_QUERY|ajaxpro       |@MissXu sorry! bed time came here (GMT+1)   http://is.gd/fNge                                                                             |2009-04-07 05:21:30|\n",
      "|39    |1467818007|2009-04-07|NO_QUERY|Anthony_Nguyen|Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my girlfriend                          |2009-04-07 05:21:39|\n",
      "|45    |1467819650|2009-04-07|NO_QUERY|antzpantz     |@Viennah Yay! I'm happy for you with your job! But that also means less time for me and you...                                            |2009-04-07 05:22:05|\n",
      "|46    |1467819712|2009-04-07|NO_QUERY|labrt2004     |Just checked my user timeline on my blackberry, it looks like the twanking is still happening  Are ppl still having probs w/ BGs and UIDs?|2009-04-07 05:22:06|\n",
      "|52    |1467821085|2009-04-07|NO_QUERY|crzy_cdn_bulas|our duck and chicken are taking wayyy too long to hatch                                                                                   |2009-04-07 05:22:26|\n",
      "|56    |1467822384|2009-04-07|NO_QUERY|Lindsey0920   |@oanhLove I hate when that happens...                                                                                                     |2009-04-07 05:22:47|\n",
      "|800007|1467822465|2009-04-07|NO_QUERY|jessicavaliyi |@KeepinUpWKris I am so jealous, hope you had a great time in vegas! how did you like the ACM's?! LOVE YOUR SHOW!!                         |2009-04-07 05:22:48|\n",
      "|800008|1467822489|2009-04-07|NO_QUERY|emmasaur28    |@tommcfly ah, congrats mr fletcher for finally joining twitter                                                                            |2009-04-07 05:22:49|\n",
      "+------+----------+----------+--------+--------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 66:======================================>                   (4 + 2) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame named df\n",
    "df_with_date.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7468c4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[Stage 71:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|text                                                                                                                                      |sentiment_score|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|@alydesigns i was out most of the day so didn't get much done                                                                             |0.0            |\n",
      "|@angry_barista I baked you a cake but I ated it                                                                                           |0.0            |\n",
      "|@MissXu sorry! bed time came here (GMT+1)   http://is.gd/fNge                                                                             |-0.1511        |\n",
      "|Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my girlfriend                          |-0.1531        |\n",
      "|@Viennah Yay! I'm happy for you with your job! But that also means less time for me and you...                                            |0.629          |\n",
      "|Just checked my user timeline on my blackberry, it looks like the twanking is still happening  Are ppl still having probs w/ BGs and UIDs?|0.3612         |\n",
      "|our duck and chicken are taking wayyy too long to hatch                                                                                   |0.0            |\n",
      "|@oanhLove I hate when that happens...                                                                                                     |-0.5719        |\n",
      "|@KeepinUpWKris I am so jealous, hope you had a great time in vegas! how did you like the ACM's?! LOVE YOUR SHOW!!                         |0.9188         |\n",
      "|@tommcfly ah, congrats mr fletcher for finally joining twitter                                                                            |0.5267         |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Preprocess text data\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "df_with_date = tokenizer.transform(df_with_date)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "df_with_date = remover.transform(df_with_date)\n",
    "\n",
    "# Calculate sentiment scores\n",
    "def calculate_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "    return sentiment_score\n",
    "\n",
    "calculate_sentiment_udf = udf(calculate_sentiment, FloatType())\n",
    "df_with_date = df_with_date.withColumn(\"sentiment_score\", calculate_sentiment_udf(col(\"text\")))\n",
    "\n",
    "# Show the DataFrame with preprocessed text data and sentiment scores\n",
    "df_with_date.select(\"text\", \"sentiment_score\").show(10,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4513555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: int, ids: bigint, date: date, flag: string, user: string, text: string, timestamp: timestamp, tokens: array<string>, filtered_tokens: array<string>, sentiment_score: float]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fd231b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:=============================>                            (3 + 3) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+--------+--------------+--------------------+-------------------+--------------------+--------------------+---------------+\n",
      "| index|       ids|      date|    flag|          user|                text|          timestamp|              tokens|     filtered_tokens|sentiment_score|\n",
      "+------+----------+----------+--------+--------------+--------------------+-------------------+--------------------+--------------------+---------------+\n",
      "|    20|1467813985|2009-04-07|NO_QUERY|        quanvu|@alydesigns i was...|2009-04-07 05:20:37|[@alydesigns, i, ...|[@alydesigns, day...|            0.0|\n",
      "|    22|1467814119|2009-04-07|NO_QUERY|     cooliodoc|@angry_barista I ...|2009-04-07 05:20:40|[@angry_barista, ...|[@angry_barista, ...|            0.0|\n",
      "|    37|1467817374|2009-04-07|NO_QUERY|       ajaxpro|@MissXu sorry! be...|2009-04-07 05:21:30|[@missxu, sorry!,...|[@missxu, sorry!,...|        -0.1511|\n",
      "|    39|1467818007|2009-04-07|NO_QUERY|Anthony_Nguyen|Bed. Class 8-12. ...|2009-04-07 05:21:39|[bed., class, 8-1...|[bed., class, 8-1...|        -0.1531|\n",
      "|    45|1467819650|2009-04-07|NO_QUERY|     antzpantz|@Viennah Yay! I'm...|2009-04-07 05:22:05|[@viennah, yay!, ...|[@viennah, yay!, ...|          0.629|\n",
      "|    46|1467819712|2009-04-07|NO_QUERY|     labrt2004|Just checked my u...|2009-04-07 05:22:06|[just, checked, m...|[checked, user, t...|         0.3612|\n",
      "|    52|1467821085|2009-04-07|NO_QUERY|crzy_cdn_bulas|our duck and chic...|2009-04-07 05:22:26|[our, duck, and, ...|[duck, chicken, t...|            0.0|\n",
      "|    56|1467822384|2009-04-07|NO_QUERY|   Lindsey0920|@oanhLove I hate ...|2009-04-07 05:22:47|[@oanhlove, i, ha...|[@oanhlove, hate,...|        -0.5719|\n",
      "|800007|1467822465|2009-04-07|NO_QUERY| jessicavaliyi|@KeepinUpWKris I ...|2009-04-07 05:22:48|[@keepinupwkris, ...|[@keepinupwkris, ...|         0.9188|\n",
      "|800008|1467822489|2009-04-07|NO_QUERY|    emmasaur28|@tommcfly ah, con...|2009-04-07 05:22:49|[@tommcfly, ah,, ...|[@tommcfly, ah,, ...|         0.5267|\n",
      "+------+----------+----------+--------+--------------+--------------------+-------------------+--------------------+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 74:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_with_date.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00955f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_values = df_with_date.select(\"date\").distinct().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c4da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'tokens' column, baceause hdfs does not accept array types:\n",
    "df_with_date_no_tokens = df_with_date.drop('tokens', 'filtered_tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output path\n",
    "output_path = \"hdfs://localhost:9000/db_to_forecast_ca2\"\n",
    "\n",
    "# upload csv into hdfs in parts, after merge to use it:\n",
    "#df_with_date_no_tokens.write.mode('overwrite').csv(output_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f67f1956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort the list of dates\n",
    "#sorted_dates = sorted(unique_values)\n",
    "\n",
    "# Print the sorted dates\n",
    "#for date in sorted_dates:\n",
    "    #print(date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141eefbd",
   "metadata": {},
   "source": [
    "## sorted list of dates:\n",
    "\n",
    "Row(date=datetime.date(2009, 4, 7))\n",
    "Row(date=datetime.date(2009, 4, 18))\n",
    "Row(date=datetime.date(2009, 4, 19))\n",
    "Row(date=datetime.date(2009, 4, 20))\n",
    "Row(date=datetime.date(2009, 4, 21))\n",
    "Row(date=datetime.date(2009, 5, 2))\n",
    "Row(date=datetime.date(2009, 5, 3))\n",
    "Row(date=datetime.date(2009, 5, 4))\n",
    "Row(date=datetime.date(2009, 5, 10))\n",
    "Row(date=datetime.date(2009, 5, 11))\n",
    "Row(date=datetime.date(2009, 5, 12))\n",
    "Row(date=datetime.date(2009, 5, 14))\n",
    "Row(date=datetime.date(2009, 5, 17))\n",
    "Row(date=datetime.date(2009, 5, 18))\n",
    "Row(date=datetime.date(2009, 5, 22))\n",
    "Row(date=datetime.date(2009, 5, 24))\n",
    "Row(date=datetime.date(2009, 5, 25))\n",
    "Row(date=datetime.date(2009, 5, 27))\n",
    "Row(date=datetime.date(2009, 5, 29))\n",
    "Row(date=datetime.date(2009, 5, 30))\n",
    "Row(date=datetime.date(2009, 5, 31))\n",
    "Row(date=datetime.date(2009, 6, 1))\n",
    "Row(date=datetime.date(2009, 6, 2))\n",
    "Row(date=datetime.date(2009, 6, 3))\n",
    "Row(date=datetime.date(2009, 6, 4))\n",
    "Row(date=datetime.date(2009, 6, 5))\n",
    "Row(date=datetime.date(2009, 6, 6))\n",
    "Row(date=datetime.date(2009, 6, 7))\n",
    "Row(date=datetime.date(2009, 6, 8))\n",
    "Row(date=datetime.date(2009, 6, 14))\n",
    "Row(date=datetime.date(2009, 6, 15))\n",
    "Row(date=datetime.date(2009, 6, 16))\n",
    "Row(date=datetime.date(2009, 6, 17))\n",
    "Row(date=datetime.date(2009, 6, 18))\n",
    "Row(date=datetime.date(2009, 6, 19))\n",
    "Row(date=datetime.date(2009, 6, 20))\n",
    "Row(date=datetime.date(2009, 6, 21))\n",
    "Row(date=datetime.date(2009, 6, 22))\n",
    "Row(date=datetime.date(2009, 6, 23))\n",
    "Row(date=datetime.date(2009, 6, 24))\n",
    "Row(date=datetime.date(2009, 6, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fff14203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 81:===================>                                      (2 + 4) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1598315"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_date.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5acbc5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Grouping by date and calculating average sentiment score for each day\n",
    "#daily_sentiment = sentiment_data.groupBy(\"date\").avg(\"sentiment_score\").orderBy(\"date\")\n",
    "\n",
    "# Collecting data locally for plotting\n",
    "#dates = [row.date for row in daily_sentiment.collect()]\n",
    "#avg_sentiment_scores = [row[\"avg(sentiment_score)\"] for row in daily_sentiment.collect()]\n",
    "\n",
    "# Plotting\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#plt.plot(dates, avg_sentiment_scores)\n",
    "#plt.title(\"Average Sentiment Score Over Time\")\n",
    "#plt.xlabel(\"Date\")\n",
    "#plt.ylabel(\"Average Sentiment Score\")\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa10b64",
   "metadata": {},
   "source": [
    "![text_score_plot](./plot_text_score.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8c4f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, avg\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Interpolate missing values\n",
    "#windowSpec = Window.orderBy(\"date\")\n",
    "#df_with_interpolated = df_with_date.withColumn(\"sentiment_score\", F.when(F.isnull(col(\"sentiment_score\")), F.avg(\"sentiment_score\").over(windowSpec)).otherwise(col(\"sentiment_score\")))\n",
    "\n",
    "# Add new columns for 1 day, 3 days, and 7 days ahead\n",
    "#df_with_interpolated = df_with_interpolated.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
    "#df_with_interpolated = df_with_interpolated.alias(\"df1\")\n",
    "#df_with_interpolated = df_with_interpolated.join(df_with_interpolated.alias(\"df2\"), (col(\"df1.row_number\") - 1) == col(\"df2.row_number\"), \"left_outer\")\n",
    "\n",
    "# Select the lagged values for each lag period\n",
    "#df_with_interpolated = df_with_interpolated.select(\n",
    "#    \"df1.*\",\n",
    "#    col(\"df2.sentiment_score\").alias(\"1_day_ahead\"),\n",
    "#    col(\"df2.sentiment_score\").alias(\"3_days_ahead\"),\n",
    "#    col(\"df2.sentiment_score\").alias(\"7_days_ahead\")\n",
    "#)\n",
    "\n",
    "# Drop the row_number column\n",
    "#df_with_interpolated = df_with_interpolated.drop(\"row_number\")\n",
    "\n",
    "# Drop rows with null values in the new columns\n",
    "#df_with_interpolated = df_with_interpolated.dropna(subset=[\"1_day_ahead\", \"3_days_ahead\", \"7_days_ahead\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db492929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_with_interpolated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_with_interpolated\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_with_interpolated' is not defined"
     ]
    }
   ],
   "source": [
    "#df_with_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea920309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql import functions as F\n",
    "#from pyspark.sql.window import Window\n",
    "#from statsmodels.tsa.arima.model import ARIMA\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df_with_interpolated is your PySpark DataFrame\n",
    "\n",
    "# Split data into train and test sets for each forecast horizon\n",
    "#train_size = 0.8  # 80% of data for training, adjust as needed\n",
    "\n",
    "# Forecasting 1 day ahead\n",
    "#train_1_day, test_1_day = df_with_interpolated.randomSplit([train_size, 1-train_size], seed=42)\n",
    "#train_1_day = train_1_day.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "#test_1_day = test_1_day.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "\n",
    "# Forecasting 3 days ahead\n",
    "#train_3_days, test_3_days = df_with_interpolated.randomSplit([train_size+3, 1-(train_size+3)], seed=42)\n",
    "#train_3_days = train_3_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "#test_3_days = test_3_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "\n",
    "\n",
    "# Forecasting 3 days ahead\n",
    "#train_3_days, test_3_days = df_with_interpolated.randomSplit([train_size, 1-train_size], seed=42)\n",
    "#train_3_days = train_3_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "#test_3_days = test_3_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "\n",
    "\n",
    "# Forecasting 7 days ahead\n",
    "#train_7_days, test_7_days = df_with_interpolated.randomSplit([train_size+7, 1-(train_size+7)], seed=42)\n",
    "#train_7_days = train_7_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "#test_7_days = test_7_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "\n",
    "# Forecasting 7 days ahead\n",
    "#train_7_days, test_7_days = df_with_interpolated.randomSplit([train_size, 1-train_size], seed=42)\n",
    "#train_7_days = train_7_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "#test_7_days = test_7_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197106aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 15:08:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 15:08:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 15:08:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 15:08:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 15:08:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 15:08:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 15:08:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 15:08:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 15:08:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 15:08:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 15:08:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 15:08:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 90:>                 (0 + 6) / 7][Stage 92:>                 (0 + 0) / 7]\r"
     ]
    }
   ],
   "source": [
    "# Define function for fitting ARIMA model and making forecasts\n",
    "#def fit_arima(train_df, test_df, order):\n",
    "#    train_data = train_df.select(\"sentiment_score\").toPandas()\n",
    "#    test_data = test_df.select(\"sentiment_score\").toPandas()\n",
    "    \n",
    "#    model = ARIMA(train_data, order=order)\n",
    "#    fit_model = model.fit()\n",
    "#    forecast = fit_model.forecast(steps=len(test_data))[0]\n",
    "#    return forecast\n",
    "\n",
    "# Define ARIMA order (p,d,q)\n",
    "#order = (5, 1, 0)  # Example order, you need to choose based on your data\n",
    "\n",
    "# Fit ARIMA model and make forecasts for each forecast horizon\n",
    "#forecast_1_day = fit_arima(train_1_day, test_1_day, order)\n",
    "\n",
    "#print(\"Forecasted sentiment scores for the test set (1 day ahead):\")\n",
    "#print(forecast_1_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810373e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast_3_days = fit_arima(train_3_days, test_3_days, order)\n",
    "#forecast_7_days = fit_arima(train_7_days, test_7_days, order)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Forecasted sentiment scores for the test set (3 days ahead):\")\n",
    "#print(forecast_3_days)\n",
    "\n",
    "print(\"Forecasted sentiment scores for the test set (7 days ahead):\")\n",
    "#print(forecast_7_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
