{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea0eea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.64.6:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2225c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3db2c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5dc9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python3-dotenv # command did not work because of the message \"error: externally-managed-environment\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0483cbf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv('.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0acc1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# mysql variables\n",
    "USER_MYSQL = os.getenv('USER_MYSQL')\n",
    "PASSWORD_MYSQL = os.getenv('PASSWORD_MYSQL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c4abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Connect to MySQL database using SQLAlchemy\n",
    "link = f\"mysql+mysqlconnector://{USER_MYSQL}:{PASSWORD_MYSQL}@localhost/ca2_tweets\"\n",
    "engine = create_engine(link)\n",
    "\n",
    "# Specify the chunk size\n",
    "chunksize = 10000\n",
    "\n",
    "# Initialize the start index\n",
    "start_index = 0\n",
    "\n",
    "# loop to upload the csv into mysql: \n",
    "#for chunk in pd.read_csv(\"ProjectTweets.csv\", chunksize=chunksize):\n",
    "#    # Insert chunk into MySQL database\n",
    "#    chunk.to_sql(name=\"ProjectTweets\", con=engine, if_exists=\"append\", index=False)\n",
    "#    \n",
    "#    # Increment start index\n",
    "#    start_index += len(chunk)\n",
    "#    print(f\"Uploaded {start_index} rows\")\n",
    "\n",
    "# Dispose of the engine\n",
    "engine.dispose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad3a0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in ca2_tweets database:\n",
      "ProjectTweets\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=USER_MYSQL,\n",
    "    password=PASSWORD_MYSQL,\n",
    "    database=\"ca2_tweets\"\n",
    ")\n",
    "\n",
    "# Create a cursor object to execute SQL queries\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute SQL query to show tables\n",
    "cursor.execute(\"SHOW TABLES\")\n",
    "\n",
    "# Fetch all rows from the result set\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Display the tables\n",
    "print(\"Tables in ca2_tweets database:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "# Close cursor and MySQL connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4fcdd39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 10:47:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 1:>                                                          (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- ids: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:=================================================>         (5 + 1) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ca2\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "df = spark.read.csv(\"hdfs://localhost:9000/ca2_bd_ad/ProjectTweets.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Print DataFrame schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c96c5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+--------+---------------+--------------------+\n",
      "|index|       ids|                date|    flag|           user|                text|\n",
      "+-----+----------+--------------------+--------+---------------+--------------------+\n",
      "|    0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|    2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|    3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|    4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+-----+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d746d2",
   "metadata": {},
   "source": [
    "## EDA :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d82845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in column  index  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:===================>                                      (2 + 4) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in column  ids  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  ids\n",
      "Null values in column  date  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  date\n",
      "Null values in column  flag  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  flag\n",
      "Null values in column  user  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  user\n",
      "Null values in column  text  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  text\n"
     ]
    }
   ],
   "source": [
    "# check missing values:\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "for name in df.columns:\n",
    "    # Check if there are null values in a specific column\n",
    "    null_values_count = df.filter(col(name).isNull()).count()\n",
    "\n",
    "    # Check if there are non-null values in a specific column\n",
    "    non_null_values_count = df.filter(col(name).isNotNull()).count()\n",
    "\n",
    "    # Print the counts\n",
    "    print(\"Null values in column \",name,\" :\", null_values_count)\n",
    "    print(\"Non-null values in column \",\" :\", non_null_values_count)\n",
    "    print(\"column name is : \",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95517573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    flag|\n",
      "+--------+\n",
      "|NO_QUERY|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 39:===================>                                      (2 + 4) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "check_flag_column = df.select(\"flag\").distinct()\n",
    "check_flag_column.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e2e3ae",
   "metadata": {},
   "source": [
    "#### column \"flag\" has just 1 value, so it is not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b82d736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:=========>                                                (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|       ids|count|\n",
      "+----------+-----+\n",
      "|1468544973|    2|\n",
      "|1690908358|    2|\n",
      "|1834777946|    2|\n",
      "|1882160717|    2|\n",
      "|1965601765|    2|\n",
      "|1982434182|    2|\n",
      "|1685304801|    2|\n",
      "|1686371908|    2|\n",
      "|1957194329|    2|\n",
      "|1969964899|    2|\n",
      "|1974268607|    2|\n",
      "|1556266702|    2|\n",
      "|1752414405|    2|\n",
      "|1824843992|    2|\n",
      "|1881996107|    2|\n",
      "|1983726537|    2|\n",
      "|1973518810|    2|\n",
      "|1983706615|    2|\n",
      "|1556925184|    2|\n",
      "|1977761720|    2|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def check_repeated_values(df, column_name):\n",
    "    return df.groupBy(column_name).count().filter(col(\"count\") >= 2)\n",
    "\n",
    "repeated_values_df = check_repeated_values(df, \"ids\")\n",
    "\n",
    "# Show the DataFrame containing rows with repeated values\n",
    "repeated_values_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a07989ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:=========>                                                (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with repeated values: 1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the number of rows with repeated values\n",
    "num_repeated_rows = repeated_values_df.count()\n",
    "print(\"Number of rows with repeated values:\", num_repeated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c49029c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after removing duplicates:\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates based on the \"id\" column\n",
    "df_no_duplicates = df.dropDuplicates([\"ids\"])\n",
    "\n",
    "# Print DataFrame without duplicates\n",
    "print(\"\\nDataFrame after removing duplicates:\")\n",
    "#df_no_duplicates.show().count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "975e316a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1598315"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_duplicates.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aeb3c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 57:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|date                        |\n",
      "+----------------------------+\n",
      "|Mon Apr 06 22:20:37 PDT 2009|\n",
      "|Mon Apr 06 22:20:40 PDT 2009|\n",
      "|Mon Apr 06 22:21:30 PDT 2009|\n",
      "|Mon Apr 06 22:21:39 PDT 2009|\n",
      "|Mon Apr 06 22:22:05 PDT 2009|\n",
      "|Mon Apr 06 22:22:06 PDT 2009|\n",
      "|Mon Apr 06 22:22:26 PDT 2009|\n",
      "|Mon Apr 06 22:22:47 PDT 2009|\n",
      "|Mon Apr 06 22:22:48 PDT 2009|\n",
      "|Mon Apr 06 22:22:49 PDT 2009|\n",
      "+----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 57:=========>                                                (1 + 5) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The first 10 values, including the whole text, of the \"date\" column\n",
    "df_no_duplicates.select(\"date\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a0755bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 60:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, unix_timestamp\n",
    "\n",
    "# Set Spark configuration for timeParserPolicy\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Convert the string-formatted \"date\" column to a timestamp\n",
    "df_with_timestamp = df_no_duplicates.withColumn(\"timestamp\", unix_timestamp(\"date\", \"EEE MMM dd HH:mm:ss zzz yyyy\").cast(\"timestamp\"))\n",
    "\n",
    "# Convert the timestamp to date format\n",
    "df_with_date = df_with_timestamp.withColumn(\"date\", to_date(\"timestamp\"))\n",
    "\n",
    "# Show the first 10 values of the \"date\" column after conversion\n",
    "df_with_date.select(\"date\").show(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdf145a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- ids: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confirm the conversion to date type in column: \n",
    "df_with_date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bedc7e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:=========>                                                (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+--------+--------------+--------------------+-------------------+\n",
      "|index|       ids|      date|    flag|          user|                text|          timestamp|\n",
      "+-----+----------+----------+--------+--------------+--------------------+-------------------+\n",
      "|   20|1467813985|2009-04-07|NO_QUERY|        quanvu|@alydesigns i was...|2009-04-07 05:20:37|\n",
      "|   22|1467814119|2009-04-07|NO_QUERY|     cooliodoc|@angry_barista I ...|2009-04-07 05:20:40|\n",
      "|   37|1467817374|2009-04-07|NO_QUERY|       ajaxpro|@MissXu sorry! be...|2009-04-07 05:21:30|\n",
      "|   39|1467818007|2009-04-07|NO_QUERY|Anthony_Nguyen|Bed. Class 8-12. ...|2009-04-07 05:21:39|\n",
      "|   45|1467819650|2009-04-07|NO_QUERY|     antzpantz|@Viennah Yay! I'm...|2009-04-07 05:22:05|\n",
      "+-----+----------+----------+--------+--------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_with_date.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef75efc",
   "metadata": {},
   "source": [
    "## column \"text\" : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b3d825b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:=========>                                                (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+--------+--------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|index |ids       |date      |flag    |user          |text                                                                                                                                      |timestamp          |\n",
      "+------+----------+----------+--------+--------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|20    |1467813985|2009-04-07|NO_QUERY|quanvu        |@alydesigns i was out most of the day so didn't get much done                                                                             |2009-04-07 05:20:37|\n",
      "|22    |1467814119|2009-04-07|NO_QUERY|cooliodoc     |@angry_barista I baked you a cake but I ated it                                                                                           |2009-04-07 05:20:40|\n",
      "|37    |1467817374|2009-04-07|NO_QUERY|ajaxpro       |@MissXu sorry! bed time came here (GMT+1)   http://is.gd/fNge                                                                             |2009-04-07 05:21:30|\n",
      "|39    |1467818007|2009-04-07|NO_QUERY|Anthony_Nguyen|Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my girlfriend                          |2009-04-07 05:21:39|\n",
      "|45    |1467819650|2009-04-07|NO_QUERY|antzpantz     |@Viennah Yay! I'm happy for you with your job! But that also means less time for me and you...                                            |2009-04-07 05:22:05|\n",
      "|46    |1467819712|2009-04-07|NO_QUERY|labrt2004     |Just checked my user timeline on my blackberry, it looks like the twanking is still happening  Are ppl still having probs w/ BGs and UIDs?|2009-04-07 05:22:06|\n",
      "|52    |1467821085|2009-04-07|NO_QUERY|crzy_cdn_bulas|our duck and chicken are taking wayyy too long to hatch                                                                                   |2009-04-07 05:22:26|\n",
      "|56    |1467822384|2009-04-07|NO_QUERY|Lindsey0920   |@oanhLove I hate when that happens...                                                                                                     |2009-04-07 05:22:47|\n",
      "|800007|1467822465|2009-04-07|NO_QUERY|jessicavaliyi |@KeepinUpWKris I am so jealous, hope you had a great time in vegas! how did you like the ACM's?! LOVE YOUR SHOW!!                         |2009-04-07 05:22:48|\n",
      "|800008|1467822489|2009-04-07|NO_QUERY|emmasaur28    |@tommcfly ah, congrats mr fletcher for finally joining twitter                                                                            |2009-04-07 05:22:49|\n",
      "+------+----------+----------+--------+--------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame named df\n",
    "df_with_date.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7468c4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[Stage 71:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|text                                                                                                                                      |sentiment_score|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|@alydesigns i was out most of the day so didn't get much done                                                                             |0.0            |\n",
      "|@angry_barista I baked you a cake but I ated it                                                                                           |0.0            |\n",
      "|@MissXu sorry! bed time came here (GMT+1)   http://is.gd/fNge                                                                             |-0.1511        |\n",
      "|Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my girlfriend                          |-0.1531        |\n",
      "|@Viennah Yay! I'm happy for you with your job! But that also means less time for me and you...                                            |0.629          |\n",
      "|Just checked my user timeline on my blackberry, it looks like the twanking is still happening  Are ppl still having probs w/ BGs and UIDs?|0.3612         |\n",
      "|our duck and chicken are taking wayyy too long to hatch                                                                                   |0.0            |\n",
      "|@oanhLove I hate when that happens...                                                                                                     |-0.5719        |\n",
      "|@KeepinUpWKris I am so jealous, hope you had a great time in vegas! how did you like the ACM's?! LOVE YOUR SHOW!!                         |0.9188         |\n",
      "|@tommcfly ah, congrats mr fletcher for finally joining twitter                                                                            |0.5267         |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Preprocess text data\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "df_with_date = tokenizer.transform(df_with_date)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "df_with_date = remover.transform(df_with_date)\n",
    "\n",
    "# Calculate sentiment scores\n",
    "def calculate_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "    return sentiment_score\n",
    "\n",
    "calculate_sentiment_udf = udf(calculate_sentiment, FloatType())\n",
    "df_with_date = df_with_date.withColumn(\"sentiment_score\", calculate_sentiment_udf(col(\"text\")))\n",
    "\n",
    "# Show the DataFrame with preprocessed text data and sentiment scores\n",
    "df_with_date.select(\"text\", \"sentiment_score\").show(10,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4513555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: int, ids: bigint, date: date, flag: string, user: string, text: string, timestamp: timestamp, tokens: array<string>, filtered_tokens: array<string>, sentiment_score: float]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fd231b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:=========>                                                (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+--------+--------------+--------------------+-------------------+--------------------+--------------------+---------------+\n",
      "| index|       ids|      date|    flag|          user|                text|          timestamp|              tokens|     filtered_tokens|sentiment_score|\n",
      "+------+----------+----------+--------+--------------+--------------------+-------------------+--------------------+--------------------+---------------+\n",
      "|    20|1467813985|2009-04-07|NO_QUERY|        quanvu|@alydesigns i was...|2009-04-07 05:20:37|[@alydesigns, i, ...|[@alydesigns, day...|            0.0|\n",
      "|    22|1467814119|2009-04-07|NO_QUERY|     cooliodoc|@angry_barista I ...|2009-04-07 05:20:40|[@angry_barista, ...|[@angry_barista, ...|            0.0|\n",
      "|    37|1467817374|2009-04-07|NO_QUERY|       ajaxpro|@MissXu sorry! be...|2009-04-07 05:21:30|[@missxu, sorry!,...|[@missxu, sorry!,...|        -0.1511|\n",
      "|    39|1467818007|2009-04-07|NO_QUERY|Anthony_Nguyen|Bed. Class 8-12. ...|2009-04-07 05:21:39|[bed., class, 8-1...|[bed., class, 8-1...|        -0.1531|\n",
      "|    45|1467819650|2009-04-07|NO_QUERY|     antzpantz|@Viennah Yay! I'm...|2009-04-07 05:22:05|[@viennah, yay!, ...|[@viennah, yay!, ...|          0.629|\n",
      "|    46|1467819712|2009-04-07|NO_QUERY|     labrt2004|Just checked my u...|2009-04-07 05:22:06|[just, checked, m...|[checked, user, t...|         0.3612|\n",
      "|    52|1467821085|2009-04-07|NO_QUERY|crzy_cdn_bulas|our duck and chic...|2009-04-07 05:22:26|[our, duck, and, ...|[duck, chicken, t...|            0.0|\n",
      "|    56|1467822384|2009-04-07|NO_QUERY|   Lindsey0920|@oanhLove I hate ...|2009-04-07 05:22:47|[@oanhlove, i, ha...|[@oanhlove, hate,...|        -0.5719|\n",
      "|800007|1467822465|2009-04-07|NO_QUERY| jessicavaliyi|@KeepinUpWKris I ...|2009-04-07 05:22:48|[@keepinupwkris, ...|[@keepinupwkris, ...|         0.9188|\n",
      "|800008|1467822489|2009-04-07|NO_QUERY|    emmasaur28|@tommcfly ah, con...|2009-04-07 05:22:49|[@tommcfly, ah,, ...|[@tommcfly, ah,, ...|         0.5267|\n",
      "+------+----------+----------+--------+--------------+--------------------+-------------------+--------------------+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 74:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_with_date.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00955f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_values = df_with_date.select(\"date\").distinct().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f67f1956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(date=datetime.date(2009, 4, 7))\n",
      "Row(date=datetime.date(2009, 4, 18))\n",
      "Row(date=datetime.date(2009, 4, 19))\n",
      "Row(date=datetime.date(2009, 4, 20))\n",
      "Row(date=datetime.date(2009, 4, 21))\n",
      "Row(date=datetime.date(2009, 5, 2))\n",
      "Row(date=datetime.date(2009, 5, 3))\n",
      "Row(date=datetime.date(2009, 5, 4))\n",
      "Row(date=datetime.date(2009, 5, 10))\n",
      "Row(date=datetime.date(2009, 5, 11))\n",
      "Row(date=datetime.date(2009, 5, 12))\n",
      "Row(date=datetime.date(2009, 5, 14))\n",
      "Row(date=datetime.date(2009, 5, 17))\n",
      "Row(date=datetime.date(2009, 5, 18))\n",
      "Row(date=datetime.date(2009, 5, 22))\n",
      "Row(date=datetime.date(2009, 5, 24))\n",
      "Row(date=datetime.date(2009, 5, 25))\n",
      "Row(date=datetime.date(2009, 5, 27))\n",
      "Row(date=datetime.date(2009, 5, 29))\n",
      "Row(date=datetime.date(2009, 5, 30))\n",
      "Row(date=datetime.date(2009, 5, 31))\n",
      "Row(date=datetime.date(2009, 6, 1))\n",
      "Row(date=datetime.date(2009, 6, 2))\n",
      "Row(date=datetime.date(2009, 6, 3))\n",
      "Row(date=datetime.date(2009, 6, 4))\n",
      "Row(date=datetime.date(2009, 6, 5))\n",
      "Row(date=datetime.date(2009, 6, 6))\n",
      "Row(date=datetime.date(2009, 6, 7))\n",
      "Row(date=datetime.date(2009, 6, 8))\n",
      "Row(date=datetime.date(2009, 6, 14))\n",
      "Row(date=datetime.date(2009, 6, 15))\n",
      "Row(date=datetime.date(2009, 6, 16))\n",
      "Row(date=datetime.date(2009, 6, 17))\n",
      "Row(date=datetime.date(2009, 6, 18))\n",
      "Row(date=datetime.date(2009, 6, 19))\n",
      "Row(date=datetime.date(2009, 6, 20))\n",
      "Row(date=datetime.date(2009, 6, 21))\n",
      "Row(date=datetime.date(2009, 6, 22))\n",
      "Row(date=datetime.date(2009, 6, 23))\n",
      "Row(date=datetime.date(2009, 6, 24))\n",
      "Row(date=datetime.date(2009, 6, 25))\n"
     ]
    }
   ],
   "source": [
    "# Sort the list of dates\n",
    "sorted_dates = sorted(unique_values)\n",
    "\n",
    "# Print the sorted dates\n",
    "for date in sorted_dates:\n",
    "    print(date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fff14203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1598315"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_date.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5acbc5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Grouping by date and calculating average sentiment score for each day\n",
    "#daily_sentiment = sentiment_data.groupBy(\"date\").avg(\"sentiment_score\").orderBy(\"date\")\n",
    "\n",
    "# Collecting data locally for plotting\n",
    "#dates = [row.date for row in daily_sentiment.collect()]\n",
    "#avg_sentiment_scores = [row[\"avg(sentiment_score)\"] for row in daily_sentiment.collect()]\n",
    "\n",
    "# Plotting\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#plt.plot(dates, avg_sentiment_scores)\n",
    "#plt.title(\"Average Sentiment Score Over Time\")\n",
    "#plt.xlabel(\"Date\")\n",
    "#plt.ylabel(\"Average Sentiment Score\")\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa10b64",
   "metadata": {},
   "source": [
    "![text_score_plot](./plot_text_score.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8c4f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, avg\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Interpolate missing values\n",
    "windowSpec = Window.orderBy(\"date\")\n",
    "df_with_interpolated = df_with_date.withColumn(\"sentiment_score\", F.when(F.isnull(col(\"sentiment_score\")), F.avg(\"sentiment_score\").over(windowSpec)).otherwise(col(\"sentiment_score\")))\n",
    "\n",
    "# Add new columns for 1 day, 3 days, and 7 days ahead\n",
    "df_with_interpolated = df_with_interpolated.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
    "df_with_interpolated = df_with_interpolated.alias(\"df1\")\n",
    "df_with_interpolated = df_with_interpolated.join(df_with_interpolated.alias(\"df2\"), (col(\"df1.row_number\") - 1) == col(\"df2.row_number\"), \"left_outer\")\n",
    "\n",
    "# Select the lagged values for each lag period\n",
    "df_with_interpolated = df_with_interpolated.select(\n",
    "    \"df1.*\",\n",
    "    col(\"df2.sentiment_score\").alias(\"1_day_ahead\"),\n",
    "    col(\"df2.sentiment_score\").alias(\"3_days_ahead\"),\n",
    "    col(\"df2.sentiment_score\").alias(\"7_days_ahead\")\n",
    ")\n",
    "\n",
    "# Drop the row_number column\n",
    "df_with_interpolated = df_with_interpolated.drop(\"row_number\")\n",
    "\n",
    "# Drop rows with null values in the new columns\n",
    "df_with_interpolated = df_with_interpolated.dropna(subset=[\"1_day_ahead\", \"3_days_ahead\", \"7_days_ahead\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db492929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: int, ids: bigint, date: date, flag: string, user: string, text: string, timestamp: timestamp, tokens: array<string>, filtered_tokens: array<string>, sentiment_score: double, 1_day_ahead: double, 3_days_ahead: double, 7_days_ahead: double]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea920309",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 11:46:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:46:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:46:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:46:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:46:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:46:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:46:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:46:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:46:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:46:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:46:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:46:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:47:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:47:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:47:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 11:47:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 12:01:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 12:01:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 12:01:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 12:01:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n",
      "24/05/16 12:40:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 12:40:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 12:40:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 12:40:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/05/16 12:40:44 ERROR TaskMemoryManager: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4ffc9129\n",
      "java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n",
      "\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:321)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.write(UnsafeSorterSpillWriter.java:136)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spillIterator(UnsafeExternalSorter.java:576)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:231)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:221)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:856)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:856)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/16 12:40:44 ERROR Executor: Exception in task 0.0 in stage 105.0 (TID 247)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4ffc9129 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:856)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:856)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/16 12:40:44 WARN TaskSetManager: Lost task 0.0 in stage 105.0 (TID 247) (192.168.64.6 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4ffc9129 : No space left on device\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\n",
      "\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:856)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:856)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/16 12:40:44 ERROR TaskSetManager: Task 0 in stage 105.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o375.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 105.0 failed 1 times, most recent failure: Lost task 0.0 in stage 105.0 (TID 247) (192.168.64.6 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4ffc9129 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)\n\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:856)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:856)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1022)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:408)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1021)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:360)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:388)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:360)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4038)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4208)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4206)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4206)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4035)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4ffc9129 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)\n\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:856)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:856)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m order \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Example order, you need to choose based on your data\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Fit ARIMA model and make forecasts for each forecast horizon\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m forecast_1_day \u001b[38;5;241m=\u001b[39m \u001b[43mfit_arima\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_1_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_1_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m forecast_3_days \u001b[38;5;241m=\u001b[39m fit_arima(train_3_days, test_3_days, order)\n\u001b[1;32m     55\u001b[0m forecast_7_days \u001b[38;5;241m=\u001b[39m fit_arima(train_7_days, test_7_days, order)\n",
      "Cell \u001b[0;32mIn[37], line 41\u001b[0m, in \u001b[0;36mfit_arima\u001b[0;34m(train_df, test_df, order)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_arima\u001b[39m(train_df, test_df, order):\n\u001b[0;32m---> 41\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentiment_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m     44\u001b[0m     model \u001b[38;5;241m=\u001b[39m ARIMA(train_data, order\u001b[38;5;241m=\u001b[39morder)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    209\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1218\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \n\u001b[1;32m   1200\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1218\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o375.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 105.0 failed 1 times, most recent failure: Lost task 0.0 in stage 105.0 (TID 247) (192.168.64.6 executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4ffc9129 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)\n\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:856)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:856)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1022)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:408)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1021)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:360)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:388)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:360)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4038)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4208)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4206)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4206)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4035)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@4ffc9129 : No space left on device\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\n\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:117)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:433)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:452)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)\n\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:856)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:856)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df_with_interpolated is your PySpark DataFrame\n",
    "\n",
    "# Split data into train and test sets for each forecast horizon\n",
    "train_size = 0.8  # 80% of data for training, adjust as needed\n",
    "\n",
    "# Forecasting 1 day ahead\n",
    "train_1_day, test_1_day = df_with_interpolated.randomSplit([train_size, 1-train_size], seed=42)\n",
    "train_1_day = train_1_day.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "test_1_day = test_1_day.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "\n",
    "# Forecasting 3 days ahead\n",
    "#train_3_days, test_3_days = df_with_interpolated.randomSplit([train_size+3, 1-(train_size+3)], seed=42)\n",
    "#train_3_days = train_3_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "#test_3_days = test_3_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "\n",
    "\n",
    "# Forecasting 3 days ahead\n",
    "train_3_days, test_3_days = df_with_interpolated.randomSplit([train_size, 1-train_size], seed=42)\n",
    "train_3_days = train_3_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "test_3_days = test_3_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "\n",
    "\n",
    "# Forecasting 7 days ahead\n",
    "#train_7_days, test_7_days = df_with_interpolated.randomSplit([train_size+7, 1-(train_size+7)], seed=42)\n",
    "#train_7_days = train_7_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "#test_7_days = test_7_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "\n",
    "# Forecasting 7 days ahead\n",
    "train_7_days, test_7_days = df_with_interpolated.randomSplit([train_size, 1-train_size], seed=42)\n",
    "train_7_days = train_7_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "test_7_days = test_7_days.select(\"date\", \"sentiment_score\").orderBy(\"date\")\n",
    "\n",
    "\n",
    "# Define function for fitting ARIMA model and making forecasts\n",
    "def fit_arima(train_df, test_df, order):\n",
    "    train_data = train_df.select(\"sentiment_score\").toPandas()\n",
    "    test_data = test_df.select(\"sentiment_score\").toPandas()\n",
    "    \n",
    "    model = ARIMA(train_data, order=order)\n",
    "    fit_model = model.fit()\n",
    "    forecast = fit_model.forecast(steps=len(test_data))[0]\n",
    "    return forecast\n",
    "\n",
    "# Define ARIMA order (p,d,q)\n",
    "order = (5, 1, 0)  # Example order, you need to choose based on your data\n",
    "\n",
    "# Fit ARIMA model and make forecasts for each forecast horizon\n",
    "forecast_1_day = fit_arima(train_1_day, test_1_day, order)\n",
    "forecast_3_days = fit_arima(train_3_days, test_3_days, order)\n",
    "forecast_7_days = fit_arima(train_7_days, test_7_days, order)\n",
    "\n",
    "print(\"Forecasted sentiment scores for the test set (1 day ahead):\")\n",
    "print(forecast_1_day)\n",
    "\n",
    "print(\"Forecasted sentiment scores for the test set (3 days ahead):\")\n",
    "print(forecast_3_days)\n",
    "\n",
    "print(\"Forecasted sentiment scores for the test set (7 days ahead):\")\n",
    "print(forecast_7_days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197106aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41843801",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"hello changess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c34e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
