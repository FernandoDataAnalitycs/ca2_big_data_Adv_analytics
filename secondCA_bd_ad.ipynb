{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea0eea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.64.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2225c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3db2c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5dc9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python3-dotenv # command did not work because of the message \"error: externally-managed-environment\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0483cbf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv('.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0acc1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# mysql variables\n",
    "USER_MYSQL = os.getenv('USER_MYSQL')\n",
    "PASSWORD_MYSQL = os.getenv('PASSWORD_MYSQL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c4abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Connect to MySQL database using SQLAlchemy\n",
    "link = f\"mysql+mysqlconnector://{USER_MYSQL}:{PASSWORD_MYSQL}@localhost/ca2_tweets\"\n",
    "engine = create_engine(link)\n",
    "\n",
    "# Specify the chunk size\n",
    "chunksize = 10000\n",
    "\n",
    "# Initialize the start index\n",
    "start_index = 0\n",
    "\n",
    "# loop to upload the csv into mysql: \n",
    "#for chunk in pd.read_csv(\"ProjectTweets.csv\", chunksize=chunksize):\n",
    "#    # Insert chunk into MySQL database\n",
    "#    chunk.to_sql(name=\"ProjectTweets\", con=engine, if_exists=\"append\", index=False)\n",
    "#    \n",
    "#    # Increment start index\n",
    "#    start_index += len(chunk)\n",
    "#    print(f\"Uploaded {start_index} rows\")\n",
    "\n",
    "# Dispose of the engine\n",
    "engine.dispose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad3a0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in ca2_tweets database:\n",
      "ProjectTweets\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Connect to MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=USER_MYSQL,\n",
    "    password=PASSWORD_MYSQL,\n",
    "    database=\"ca2_tweets\"\n",
    ")\n",
    "\n",
    "# Create a cursor object to execute SQL queries\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute SQL query to show tables\n",
    "cursor.execute(\"SHOW TABLES\")\n",
    "\n",
    "# Fetch all rows from the result set\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "# Display the tables\n",
    "print(\"Tables in ca2_tweets database:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "# Close cursor and MySQL connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "891f6f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: file:///home/hduser/Documents/secondCA_big_data_ad/ca2_big_data_Adv_analytics/mysql-connector-j-8.0.33.jar\n"
     ]
    }
   ],
   "source": [
    "jar_path = \"file://\" + os.getcwd() + \"/mysql-connector-j-8.0.33.jar\"\n",
    "print(\"file:\",jar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4fcdd39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/10 20:09:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "[Stage 1:>                                                          (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- ids: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ca2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "df = spark.read.csv(\"hdfs://localhost:9000/ca2_bd_ad/ProjectTweets.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Print DataFrame schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c96c5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+--------+---------------+--------------------+\n",
      "|index|       ids|                date|    flag|           user|                text|\n",
      "+-----+----------+--------------------+--------+---------------+--------------------+\n",
      "|    0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|    1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|    2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|    3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|    4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+-----+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d746d2",
   "metadata": {},
   "source": [
    "## EDA :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d82845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in column  index  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:=================================================>         (5 + 1) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in column  ids  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  ids\n",
      "Null values in column  date  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:===================>                                      (2 + 4) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in column  flag  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  flag\n",
      "Null values in column  user  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  user\n",
      "Null values in column  text  : 0\n",
      "Non-null values in column   : 1600000\n",
      "column name is :  text\n"
     ]
    }
   ],
   "source": [
    "# check missing values:\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "for name in df.columns:\n",
    "    # Check if there are null values in a specific column\n",
    "    null_values_count = df.filter(col(name).isNull()).count()\n",
    "\n",
    "    # Check if there are non-null values in a specific column\n",
    "    non_null_values_count = df.filter(col(name).isNotNull()).count()\n",
    "\n",
    "    # Print the counts\n",
    "    print(\"Null values in column \",name,\" :\", null_values_count)\n",
    "    print(\"Non-null values in column \",\" :\", non_null_values_count)\n",
    "    print(\"column name is : \",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95517573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    flag|\n",
      "+--------+\n",
      "|NO_QUERY|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_flag_column = df.select(\"flag\").distinct()\n",
    "check_flag_column.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e2e3ae",
   "metadata": {},
   "source": [
    "#### column \"flag\" has just 1 value, so it is not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b82d736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 42:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|       ids|count|\n",
      "+----------+-----+\n",
      "|1468544973|    2|\n",
      "|1690908358|    2|\n",
      "|1834777946|    2|\n",
      "|1882160717|    2|\n",
      "|1965601765|    2|\n",
      "|1982434182|    2|\n",
      "|1685304801|    2|\n",
      "|1686371908|    2|\n",
      "|1957194329|    2|\n",
      "|1969964899|    2|\n",
      "|1974268607|    2|\n",
      "|1556266702|    2|\n",
      "|1752414405|    2|\n",
      "|1824843992|    2|\n",
      "|1881996107|    2|\n",
      "|1983726537|    2|\n",
      "|1973518810|    2|\n",
      "|1983706615|    2|\n",
      "|1556925184|    2|\n",
      "|1977761720|    2|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def check_repeated_values(df, column_name):\n",
    "    #return df.groupBy(column_name).count().filter(col(column_name) > 1)\n",
    "    return df.groupBy(column_name).count().filter(col(\"count\") >= 2)\n",
    "\n",
    "#print(\"function : \",check_repeated_values(df,\"ids\"))\n",
    "\n",
    "repeated_values_df = check_repeated_values(df, \"ids\")\n",
    "\n",
    "# Show the DataFrame containing rows with repeated values\n",
    "repeated_values_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a07989ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 45:>                                                         (0 + 6) / 6]\r",
      "\r",
      "[Stage 45:======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with repeated values: 1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the number of rows with repeated values\n",
    "num_repeated_rows = repeated_values_df.count()\n",
    "print(\"Number of rows with repeated values:\", num_repeated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c49029c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after removing duplicates:\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates based on the \"id\" column\n",
    "df_no_duplicates = df.dropDuplicates([\"ids\"])\n",
    "\n",
    "# Print DataFrame without duplicates\n",
    "print(\"\\nDataFrame after removing duplicates:\")\n",
    "#df_no_duplicates.show().count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "975e316a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1598315"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_duplicates.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8aeb3c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 57:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|date                        |\n",
      "+----------------------------+\n",
      "|Mon Apr 06 22:20:37 PDT 2009|\n",
      "|Mon Apr 06 22:20:40 PDT 2009|\n",
      "|Mon Apr 06 22:21:30 PDT 2009|\n",
      "|Mon Apr 06 22:21:39 PDT 2009|\n",
      "|Mon Apr 06 22:22:05 PDT 2009|\n",
      "|Mon Apr 06 22:22:06 PDT 2009|\n",
      "|Mon Apr 06 22:22:26 PDT 2009|\n",
      "|Mon Apr 06 22:22:47 PDT 2009|\n",
      "|Mon Apr 06 22:22:48 PDT 2009|\n",
      "|Mon Apr 06 22:22:49 PDT 2009|\n",
      "+----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 57:=============================>                            (3 + 3) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The first 10 values, including the whole text, of the \"date\" column\n",
    "df_no_duplicates.select(\"date\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a0755bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 60:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "|2009-04-07|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, unix_timestamp\n",
    "\n",
    "# Set Spark configuration for timeParserPolicy\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Convert the string-formatted \"date\" column to a timestamp\n",
    "df_with_timestamp = df_no_duplicates.withColumn(\"timestamp\", unix_timestamp(\"date\", \"EEE MMM dd HH:mm:ss zzz yyyy\").cast(\"timestamp\"))\n",
    "\n",
    "# Convert the timestamp to date format\n",
    "df_with_date = df_with_timestamp.withColumn(\"date\", to_date(\"timestamp\"))\n",
    "\n",
    "# Show the first 10 values of the \"date\" column after conversion\n",
    "df_with_date.select(\"date\").show(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdf145a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- ids: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confirm the conversion to date type in column: \n",
    "df_with_date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bedc7e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 63:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+--------+--------------+--------------------+-------------------+\n",
      "|index|       ids|      date|    flag|          user|                text|          timestamp|\n",
      "+-----+----------+----------+--------+--------------+--------------------+-------------------+\n",
      "|   20|1467813985|2009-04-07|NO_QUERY|        quanvu|@alydesigns i was...|2009-04-07 05:20:37|\n",
      "|   22|1467814119|2009-04-07|NO_QUERY|     cooliodoc|@angry_barista I ...|2009-04-07 05:20:40|\n",
      "|   37|1467817374|2009-04-07|NO_QUERY|       ajaxpro|@MissXu sorry! be...|2009-04-07 05:21:30|\n",
      "|   39|1467818007|2009-04-07|NO_QUERY|Anthony_Nguyen|Bed. Class 8-12. ...|2009-04-07 05:21:39|\n",
      "|   45|1467819650|2009-04-07|NO_QUERY|     antzpantz|@Viennah Yay! I'm...|2009-04-07 05:22:05|\n",
      "+-----+----------+----------+--------+--------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_with_date.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef75efc",
   "metadata": {},
   "source": [
    "## column \"text\" : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b3d825b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 66:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+--------+--------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|index |ids       |date      |flag    |user          |text                                                                                                                                      |timestamp          |\n",
      "+------+----------+----------+--------+--------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|20    |1467813985|2009-04-07|NO_QUERY|quanvu        |@alydesigns i was out most of the day so didn't get much done                                                                             |2009-04-07 05:20:37|\n",
      "|22    |1467814119|2009-04-07|NO_QUERY|cooliodoc     |@angry_barista I baked you a cake but I ated it                                                                                           |2009-04-07 05:20:40|\n",
      "|37    |1467817374|2009-04-07|NO_QUERY|ajaxpro       |@MissXu sorry! bed time came here (GMT+1)   http://is.gd/fNge                                                                             |2009-04-07 05:21:30|\n",
      "|39    |1467818007|2009-04-07|NO_QUERY|Anthony_Nguyen|Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my girlfriend                          |2009-04-07 05:21:39|\n",
      "|45    |1467819650|2009-04-07|NO_QUERY|antzpantz     |@Viennah Yay! I'm happy for you with your job! But that also means less time for me and you...                                            |2009-04-07 05:22:05|\n",
      "|46    |1467819712|2009-04-07|NO_QUERY|labrt2004     |Just checked my user timeline on my blackberry, it looks like the twanking is still happening  Are ppl still having probs w/ BGs and UIDs?|2009-04-07 05:22:06|\n",
      "|52    |1467821085|2009-04-07|NO_QUERY|crzy_cdn_bulas|our duck and chicken are taking wayyy too long to hatch                                                                                   |2009-04-07 05:22:26|\n",
      "|56    |1467822384|2009-04-07|NO_QUERY|Lindsey0920   |@oanhLove I hate when that happens...                                                                                                     |2009-04-07 05:22:47|\n",
      "|800007|1467822465|2009-04-07|NO_QUERY|jessicavaliyi |@KeepinUpWKris I am so jealous, hope you had a great time in vegas! how did you like the ACM's?! LOVE YOUR SHOW!!                         |2009-04-07 05:22:48|\n",
      "|800008|1467822489|2009-04-07|NO_QUERY|emmasaur28    |@tommcfly ah, congrats mr fletcher for finally joining twitter                                                                            |2009-04-07 05:22:49|\n",
      "+------+----------+----------+--------+--------------+------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Assuming you have a DataFrame named df\n",
    "df_with_date.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db79185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_date_ = df_with_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7468c4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[Stage 69:======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|text                                                                                                                                      |sentiment_score|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "|@alydesigns i was out most of the day so didn't get much done                                                                             |0.0            |\n",
      "|@angry_barista I baked you a cake but I ated it                                                                                           |0.0            |\n",
      "|@MissXu sorry! bed time came here (GMT+1)   http://is.gd/fNge                                                                             |-0.1511        |\n",
      "|Bed. Class 8-12. Work 12-3. Gym 3-5 or 6. Then class 6-10. Another day that's gonna fly by. I miss my girlfriend                          |-0.1531        |\n",
      "|@Viennah Yay! I'm happy for you with your job! But that also means less time for me and you...                                            |0.629          |\n",
      "|Just checked my user timeline on my blackberry, it looks like the twanking is still happening  Are ppl still having probs w/ BGs and UIDs?|0.3612         |\n",
      "|our duck and chicken are taking wayyy too long to hatch                                                                                   |0.0            |\n",
      "|@oanhLove I hate when that happens...                                                                                                     |-0.5719        |\n",
      "|@KeepinUpWKris I am so jealous, hope you had a great time in vegas! how did you like the ACM's?! LOVE YOUR SHOW!!                         |0.9188         |\n",
      "|@tommcfly ah, congrats mr fletcher for finally joining twitter                                                                            |0.5267         |\n",
      "|@e4VoIP I RESPONDED  Stupid cat is helping me type. Forgive errors                                                                        |-0.3612        |\n",
      "|Ugh....92 degrees tomorrow                                                                                                                |0.0            |\n",
      "|crazy day of school. there for 10 hours straiiight. about to watch the hills. @spencerpratt told me too! ha. happy birthday JB!           |0.6467         |\n",
      "|@michellardi i really don't know. i think its Globe!  yeah! sana gumaling na ko para alam ko na din kung makakasama ako! )                |0.4724         |\n",
      "|Catching Up on Emails, RSS and Random BACN. Then I'm cutting out early tonight (11:30PM) to have Dinner with @lauraw                      |-0.128         |\n",
      "|Going to bed so goodnight everyone  and sweet dreams  http://twitpic.com/2y2e0                                                            |0.6908         |\n",
      "|The Life is cool. But not for Me.                                                                                                         |0.1655         |\n",
      "|@wisdomous you're welcome. glad you enjoyed it.                                                                                           |0.8519         |\n",
      "|YAY! found a new cuddle buddy                                                                                                             |0.6625         |\n",
      "|having a long conversation with mom on phone                                                                                              |0.0            |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 71:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Preprocess text data\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "df_with_date_ = tokenizer.transform(df_with_date_)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "df_with_date_ = remover.transform(df_with_date_)\n",
    "\n",
    "# Calculate sentiment scores\n",
    "def calculate_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
    "    return sentiment_score\n",
    "\n",
    "calculate_sentiment_udf = udf(calculate_sentiment, FloatType())\n",
    "df_with_date_ = df_with_date_.withColumn(\"sentiment_score\", calculate_sentiment_udf(col(\"text\")))\n",
    "\n",
    "# Show the DataFrame with preprocessed text data and sentiment scores\n",
    "df_with_date_.select(\"text\", \"sentiment_score\").show(truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "#spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4513555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: int, ids: bigint, date: date, flag: string, user: string, text: string, timestamp: timestamp, tokens: array<string>, filtered_tokens: array<string>, sentiment_score: float]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_date_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fd231b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:===================>                                      (2 + 4) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+--------+--------------+--------------------+-------------------+--------------------+--------------------+---------------+\n",
      "| index|       ids|      date|    flag|          user|                text|          timestamp|              tokens|     filtered_tokens|sentiment_score|\n",
      "+------+----------+----------+--------+--------------+--------------------+-------------------+--------------------+--------------------+---------------+\n",
      "|    20|1467813985|2009-04-07|NO_QUERY|        quanvu|@alydesigns i was...|2009-04-07 05:20:37|[@alydesigns, i, ...|[@alydesigns, day...|            0.0|\n",
      "|    22|1467814119|2009-04-07|NO_QUERY|     cooliodoc|@angry_barista I ...|2009-04-07 05:20:40|[@angry_barista, ...|[@angry_barista, ...|            0.0|\n",
      "|    37|1467817374|2009-04-07|NO_QUERY|       ajaxpro|@MissXu sorry! be...|2009-04-07 05:21:30|[@missxu, sorry!,...|[@missxu, sorry!,...|        -0.1511|\n",
      "|    39|1467818007|2009-04-07|NO_QUERY|Anthony_Nguyen|Bed. Class 8-12. ...|2009-04-07 05:21:39|[bed., class, 8-1...|[bed., class, 8-1...|        -0.1531|\n",
      "|    45|1467819650|2009-04-07|NO_QUERY|     antzpantz|@Viennah Yay! I'm...|2009-04-07 05:22:05|[@viennah, yay!, ...|[@viennah, yay!, ...|          0.629|\n",
      "|    46|1467819712|2009-04-07|NO_QUERY|     labrt2004|Just checked my u...|2009-04-07 05:22:06|[just, checked, m...|[checked, user, t...|         0.3612|\n",
      "|    52|1467821085|2009-04-07|NO_QUERY|crzy_cdn_bulas|our duck and chic...|2009-04-07 05:22:26|[our, duck, and, ...|[duck, chicken, t...|            0.0|\n",
      "|    56|1467822384|2009-04-07|NO_QUERY|   Lindsey0920|@oanhLove I hate ...|2009-04-07 05:22:47|[@oanhlove, i, ha...|[@oanhlove, hate,...|        -0.5719|\n",
      "|800007|1467822465|2009-04-07|NO_QUERY| jessicavaliyi|@KeepinUpWKris I ...|2009-04-07 05:22:48|[@keepinupwkris, ...|[@keepinupwkris, ...|         0.9188|\n",
      "|800008|1467822489|2009-04-07|NO_QUERY|    emmasaur28|@tommcfly ah, con...|2009-04-07 05:22:49|[@tommcfly, ah,, ...|[@tommcfly, ah,, ...|         0.5267|\n",
      "+------+----------+----------+--------+--------------+--------------------+-------------------+--------------------+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_with_date_.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00955f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_values = df_with_date_.select(\"date\").distinct().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43679853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-06-25\n",
      "2009-06-15\n",
      "2009-06-07\n",
      "2009-05-03\n",
      "2009-06-24\n",
      "2009-05-11\n",
      "2009-05-27\n",
      "2009-06-20\n",
      "2009-04-20\n",
      "2009-06-22\n",
      "2009-04-21\n",
      "2009-05-25\n",
      "2009-06-01\n",
      "2009-06-08\n",
      "2009-04-19\n",
      "2009-05-18\n",
      "2009-06-21\n",
      "2009-06-04\n",
      "2009-06-19\n",
      "2009-06-16\n",
      "2009-05-31\n",
      "2009-05-22\n",
      "2009-05-30\n",
      "2009-05-14\n",
      "2009-04-07\n",
      "2009-06-03\n",
      "2009-05-12\n",
      "2009-05-24\n",
      "2009-06-02\n",
      "2009-06-17\n",
      "2009-05-04\n",
      "2009-05-02\n",
      "2009-05-10\n",
      "2009-04-18\n",
      "2009-06-05\n",
      "2009-06-06\n",
      "2009-05-29\n",
      "2009-05-17\n",
      "2009-06-14\n",
      "2009-06-23\n",
      "2009-06-18\n"
     ]
    }
   ],
   "source": [
    "for row in unique_values:\n",
    "    print(row[0])  # Assuming 'your_column' is the first column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f67f1956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(date=datetime.date(2009, 4, 7))\n",
      "Row(date=datetime.date(2009, 4, 18))\n",
      "Row(date=datetime.date(2009, 4, 19))\n",
      "Row(date=datetime.date(2009, 4, 20))\n",
      "Row(date=datetime.date(2009, 4, 21))\n",
      "Row(date=datetime.date(2009, 5, 2))\n",
      "Row(date=datetime.date(2009, 5, 3))\n",
      "Row(date=datetime.date(2009, 5, 4))\n",
      "Row(date=datetime.date(2009, 5, 10))\n",
      "Row(date=datetime.date(2009, 5, 11))\n",
      "Row(date=datetime.date(2009, 5, 12))\n",
      "Row(date=datetime.date(2009, 5, 14))\n",
      "Row(date=datetime.date(2009, 5, 17))\n",
      "Row(date=datetime.date(2009, 5, 18))\n",
      "Row(date=datetime.date(2009, 5, 22))\n",
      "Row(date=datetime.date(2009, 5, 24))\n",
      "Row(date=datetime.date(2009, 5, 25))\n",
      "Row(date=datetime.date(2009, 5, 27))\n",
      "Row(date=datetime.date(2009, 5, 29))\n",
      "Row(date=datetime.date(2009, 5, 30))\n",
      "Row(date=datetime.date(2009, 5, 31))\n",
      "Row(date=datetime.date(2009, 6, 1))\n",
      "Row(date=datetime.date(2009, 6, 2))\n",
      "Row(date=datetime.date(2009, 6, 3))\n",
      "Row(date=datetime.date(2009, 6, 4))\n",
      "Row(date=datetime.date(2009, 6, 5))\n",
      "Row(date=datetime.date(2009, 6, 6))\n",
      "Row(date=datetime.date(2009, 6, 7))\n",
      "Row(date=datetime.date(2009, 6, 8))\n",
      "Row(date=datetime.date(2009, 6, 14))\n",
      "Row(date=datetime.date(2009, 6, 15))\n",
      "Row(date=datetime.date(2009, 6, 16))\n",
      "Row(date=datetime.date(2009, 6, 17))\n",
      "Row(date=datetime.date(2009, 6, 18))\n",
      "Row(date=datetime.date(2009, 6, 19))\n",
      "Row(date=datetime.date(2009, 6, 20))\n",
      "Row(date=datetime.date(2009, 6, 21))\n",
      "Row(date=datetime.date(2009, 6, 22))\n",
      "Row(date=datetime.date(2009, 6, 23))\n",
      "Row(date=datetime.date(2009, 6, 24))\n",
      "Row(date=datetime.date(2009, 6, 25))\n"
     ]
    }
   ],
   "source": [
    "# Assuming unique_values is a list of date strings\n",
    "# If not, you may need to convert them to date strings first\n",
    "\n",
    "# Sort the list of dates\n",
    "sorted_dates = sorted(unique_values)\n",
    "\n",
    "# Print the sorted dates\n",
    "for date in sorted_dates:\n",
    "    print(date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fff14203",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1598315"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_date_.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41843801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello changes\n"
     ]
    }
   ],
   "source": [
    "print(\"hello changess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c34e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
